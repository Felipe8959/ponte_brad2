{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6601631e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\felip\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\felip\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\felip\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\felip\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\felip\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.7.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\felip\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.66.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\felip\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\felip\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\felip\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\felip\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\felip\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\felip\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\felip\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.33.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\felip\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\felip\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\felip\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\felip\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\felip\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\felip\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\felip\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\felip\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\felip\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\felip\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\felip\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\felip\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\felip\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\felip\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\felip\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\felip\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\felip\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2024.7.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers pandas numpy faiss-cpu tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f35b68cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\felip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\felip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\felip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\felip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\felip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\felip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\felip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\felip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\felip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\felip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\felip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\felip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\felip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 546, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\felip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\felip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\felip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\felip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\felip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\felip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\felip\\AppData\\Local\\Temp\\ipykernel_15500\\2955118475.py\", line 9, in <module>\n",
      "    import faiss\n",
      "  File \"c:\\Users\\felip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\faiss\\__init__.py\", line 32, in <module>\n",
      "    from .loader import *\n",
      "  File \"c:\\Users\\felip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\faiss\\loader.py\", line 65, in <module>\n",
      "    from .swigfaiss import *\n",
      "  File \"c:\\Users\\felip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\faiss\\swigfaiss.py\", line 10, in <module>\n",
      "    from . import _swigfaiss\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aviso: FAISS não encontrado. Usando multiplicação de matrizes (mais lento, mas funciona).\n",
      "Carregando modelo de: ./bert_original\n",
      "Dispositivo de processamento: CPU\n",
      "Gerando dados de exemplo (40k manif, 5k categ)...\n",
      "Preparando hierarquia das categorias...\n",
      "\n",
      "1. Vetorizando 200 Categorias...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando Embeddings: 100%|██████████| 7/7 [00:03<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Vetorizando 100 Reclamações...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando Embeddings: 100%|██████████| 4/4 [00:01<00:00,  2.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Realizando o Matching (Cruzamento)...\n",
      "Usando Numpy Dot Product...\n",
      "Atribuindo resultados ao dataframe...\n",
      "\n",
      "--- AMOSTRA DO RESULTADO ---\n",
      "                                      TEXTO_CLIENTE  FAMILIA  PRODUTO  \\\n",
      "0  O app do banco travou e não consigo pagar boleto  Cartões  Crédito   \n",
      "1  O app do banco travou e não consigo pagar boleto  Cartões  Crédito   \n",
      "2  O app do banco travou e não consigo pagar boleto  Cartões  Crédito   \n",
      "3  O app do banco travou e não consigo pagar boleto  Cartões  Crédito   \n",
      "4  O app do banco travou e não consigo pagar boleto  Cartões  Crédito   \n",
      "\n",
      "      ASSUNTO  SCORE_CONFIANCA  \n",
      "0  Problema 1          0.57154  \n",
      "1  Problema 1          0.57154  \n",
      "2  Problema 1          0.57154  \n",
      "3  Problema 1          0.57154  \n",
      "4  Problema 1          0.57154  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm  # Barra de progresso\n",
    "\n",
    "# Tenta importar FAISS, senão usa fallback\n",
    "try:\n",
    "    import faiss\n",
    "    USE_FAISS = True\n",
    "except ImportError:\n",
    "    USE_FAISS = False\n",
    "    print(\"Aviso: FAISS não encontrado. Usando multiplicação de matrizes (mais lento, mas funciona).\")\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURAÇÕES E CARREGAMENTO\n",
    "# ==========================================\n",
    "MODEL_PATH = \"./bert_original\"  # Pasta onde você reconstruiu o modelo\n",
    "BATCH_SIZE = 32              # Processar 32 textos por vez (ajuste conforme memória RAM)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Carregando modelo de: {MODEL_PATH}\")\n",
    "print(f\"Dispositivo de processamento: {DEVICE.upper()}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModel.from_pretrained(MODEL_PATH).to(DEVICE)\n",
    "model.eval() # Modo de avaliação (mais rápido, desliga dropout)\n",
    "\n",
    "# ==========================================\n",
    "# 2. FUNÇÕES OTIMIZADAS\n",
    "# ==========================================\n",
    "\n",
    "def generate_embeddings_batched(texts, tokenizer, model, batch_size=32):\n",
    "    \"\"\"Gera embeddings em lotes para não estourar a memória e ser rápido.\"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    # Loop com barra de progresso\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Gerando Embeddings\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        \n",
    "        # Tokenização\n",
    "        inputs = tokenizer(\n",
    "            batch_texts, \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=128, \n",
    "            return_tensors=\"pt\"\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        # Mean Pooling (Média da última camada)\n",
    "        # Atenção: Mover para CPU para liberar VRAM/RAM\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "        \n",
    "        # Normalização L2 (Essencial para Cosseno funcionar com FAISS/MatMul)\n",
    "        # Isso faz com que a Distância Euclidiana seja equivalente à Similaridade de Cosseno\n",
    "        faiss.normalize_L2(embeddings) if USE_FAISS else None\n",
    "        if not USE_FAISS:\n",
    "            norm = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "            embeddings = embeddings / (norm + 1e-10)\n",
    "            \n",
    "        all_embeddings.append(embeddings)\n",
    "        \n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "# ==========================================\n",
    "# 3. PREPARAÇÃO DOS DADOS\n",
    "# ==========================================\n",
    "\n",
    "# --- A) Carregar seus Dataframes reais aqui ---\n",
    "# df_categ = pd.read_csv(\"sua_base_categorias.csv\")\n",
    "# df_manif = pd.read_csv(\"sua_base_reclamacoes.csv\")\n",
    "\n",
    "# ... Para o exemplo, vou criar dados dummy na escala que você falou ...\n",
    "print(\"Gerando dados de exemplo (40k manif, 5k categ)...\")\n",
    "df_categ = pd.DataFrame({\n",
    "    'FAMILIA': ['Cartões'] * 100 + ['Conta'] * 100,\n",
    "    'PRODUTO': ['Crédito'] * 100 + ['Corrente'] * 100,\n",
    "    'ASSUNTO': [f'Problema {i}' for i in range(200)]\n",
    "})\n",
    "df_manif = pd.DataFrame({\n",
    "    'ID': range(100),\n",
    "    'TEXTO_CLIENTE': [\"O app do banco travou e não consigo pagar boleto\"] * 100\n",
    "})\n",
    "\n",
    "# --- B) Criar \"Assinatura Hierárquica\" das Categorias ---\n",
    "print(\"Preparando hierarquia das categorias...\")\n",
    "df_categ['texto_full'] = df_categ['FAMILIA'] + \" \" + df_categ['PRODUTO'] + \" \" + df_categ['ASSUNTO']\n",
    "\n",
    "# ==========================================\n",
    "# 4. GERAÇÃO DOS VETORES (A parte pesada)\n",
    "# ==========================================\n",
    "\n",
    "print(f\"\\n1. Vetorizando {len(df_categ)} Categorias...\")\n",
    "categ_vectors = generate_embeddings_batched(df_categ['texto_full'].tolist(), tokenizer, model, BATCH_SIZE)\n",
    "\n",
    "print(f\"\\n2. Vetorizando {len(df_manif)} Reclamações...\")\n",
    "manif_vectors = generate_embeddings_batched(df_manif['TEXTO_CLIENTE'].tolist(), tokenizer, model, BATCH_SIZE)\n",
    "\n",
    "# ==========================================\n",
    "# 5. BUSCA DE SIMILARIDADE (Otimização FAISS)\n",
    "# ==========================================\n",
    "print(\"\\n3. Realizando o Matching (Cruzamento)...\")\n",
    "\n",
    "# Índices das melhores categorias\n",
    "best_indices = []\n",
    "confidence_scores = []\n",
    "\n",
    "if USE_FAISS:\n",
    "    # Cria um índice Flat (busca exata) com produto interno (IP)\n",
    "    # Como os vetores estão normalizados, IP == Cosseno\n",
    "    dimension = categ_vectors.shape[1]\n",
    "    index = faiss.IndexFlatIP(dimension)\n",
    "    \n",
    "    # Adiciona as categorias ao \"banco de dados\" do FAISS\n",
    "    index.add(categ_vectors)\n",
    "    \n",
    "    # Busca os 1 vizinhos mais próximos para cada reclamação\n",
    "    # k=1 retorna a melhor categoria\n",
    "    D, I = index.search(manif_vectors, k=1)\n",
    "    \n",
    "    best_indices = I.flatten()\n",
    "    confidence_scores = D.flatten()\n",
    "\n",
    "else:\n",
    "    # Fallback: Multiplicação de Matrizes (Rápido, mas consome muita RAM)\n",
    "    # Similaridade = A . B^T\n",
    "    print(\"Usando Numpy Dot Product...\")\n",
    "    sim_matrix = np.dot(manif_vectors, categ_vectors.T)\n",
    "    best_indices = np.argmax(sim_matrix, axis=1)\n",
    "    confidence_scores = np.max(sim_matrix, axis=1)\n",
    "\n",
    "# ==========================================\n",
    "# 6. ATRIBUIÇÃO DOS RESULTADOS\n",
    "# ==========================================\n",
    "\n",
    "print(\"Atribuindo resultados ao dataframe...\")\n",
    "\n",
    "# Usamos os índices encontrados para buscar as linhas correspondentes no df_categ\n",
    "matched_categories = df_categ.iloc[best_indices].reset_index(drop=True)\n",
    "\n",
    "# Acopla ao dataframe original\n",
    "df_final = pd.concat([df_manif, matched_categories[['FAMILIA', 'PRODUTO', 'ASSUNTO']]], axis=1)\n",
    "df_final['SCORE_CONFIANCA'] = confidence_scores\n",
    "\n",
    "# Opcional: Filtrar apenas classificações com confiança alta\n",
    "# df_final = df_final[df_final['SCORE_CONFIANCA'] > 0.85]\n",
    "\n",
    "print(\"\\n--- AMOSTRA DO RESULTADO ---\")\n",
    "print(df_final[['TEXTO_CLIENTE', 'FAMILIA', 'PRODUTO', 'ASSUNTO', 'SCORE_CONFIANCA']].head())\n",
    "\n",
    "# Salvar\n",
    "# df_final.to_csv(\"resultado_classificacao.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15be547",
   "metadata": {},
   "source": [
    "## Utilizando BERT + Histórico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f994e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gerando vetores do histórico (Isso o BERT faz bem)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando Embeddings: 100%|██████████| 19/19 [00:07<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinando o cérebro da categorização...\n",
      "Iteration 1, loss = 1.59269702\n",
      "Iteration 2, loss = 1.44265938\n",
      "Iteration 3, loss = 1.29678345\n",
      "Iteration 4, loss = 1.13392746\n",
      "Iteration 5, loss = 0.95330161\n",
      "Iteration 6, loss = 0.76430970\n",
      "Iteration 7, loss = 0.58441502\n",
      "Iteration 8, loss = 0.42137763\n",
      "Iteration 9, loss = 0.28783959\n",
      "Iteration 10, loss = 0.18811737\n",
      "Iteration 11, loss = 0.11828376\n",
      "Iteration 12, loss = 0.07396536\n",
      "Iteration 13, loss = 0.04704050\n",
      "Iteration 14, loss = 0.03078639\n",
      "Iteration 15, loss = 0.02099532\n",
      "Iteration 16, loss = 0.01498709\n",
      "Iteration 17, loss = 0.01116718\n",
      "Iteration 18, loss = 0.00863339\n",
      "Iteration 19, loss = 0.00698980\n",
      "Iteration 20, loss = 0.00583236\n",
      "Iteration 21, loss = 0.00501481\n",
      "Iteration 22, loss = 0.00440206\n",
      "Iteration 23, loss = 0.00394072\n",
      "Iteration 24, loss = 0.00358197\n",
      "Iteration 25, loss = 0.00329869\n",
      "Iteration 26, loss = 0.00306795\n",
      "Iteration 27, loss = 0.00287312\n",
      "Iteration 28, loss = 0.00270872\n",
      "Iteration 29, loss = 0.00256682\n",
      "Iteration 30, loss = 0.00244372\n",
      "Iteration 31, loss = 0.00233366\n",
      "Iteration 32, loss = 0.00223708\n",
      "Iteration 33, loss = 0.00214518\n",
      "Iteration 34, loss = 0.00206318\n",
      "Iteration 35, loss = 0.00198708\n",
      "Iteration 36, loss = 0.00191719\n",
      "Iteration 37, loss = 0.00185042\n",
      "Iteration 38, loss = 0.00178873\n",
      "Iteration 39, loss = 0.00173039\n",
      "Iteration 40, loss = 0.00167477\n",
      "Iteration 41, loss = 0.00162269\n",
      "Iteration 42, loss = 0.00157348\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "\n",
      "Relatório de Qualidade do Modelo:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "   Atendimento - Postura       1.00      1.00      1.00        44\n",
      "       Cartão - Cobrança       1.00      1.00      1.00        20\n",
      "          Cartão - Falha       1.00      1.00      1.00        16\n",
      "             Conta - Pix       1.00      1.00      1.00        18\n",
      "Investimento - Aplicação       1.00      1.00      1.00        22\n",
      "\n",
      "                accuracy                           1.00       120\n",
      "               macro avg       1.00      1.00      1.00       120\n",
      "            weighted avg       1.00      1.00      1.00       120\n",
      "\n",
      "Classificador salvo com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier # Rede neural leve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib # Para salvar o modelo treinado\n",
    "\n",
    "# ======================================================\n",
    "# 1. CARREGAR DADOS E GERAR VETORES (Como antes)\n",
    "# ======================================================\n",
    "# Supondo que você já rodou o código anterior e tem a função 'generate_embeddings_batched'\n",
    "# e o modelo BERT carregado.\n",
    "\n",
    "# df_historico = pd.read_csv(\"historico_classificado.csv\")\n",
    "# Exemplo simulado:\n",
    "df_historico = pd.DataFrame({\n",
    "    'TEXTO_CLIENTE': [\n",
    "        \"cobrança indevida no cartão\", \"pix não chegou\", \"atendente gritou comigo\", \n",
    "        \"cartão não passa\", \"quero investir no cdb\", \"gerente destratou\"\n",
    "    ] * 100,\n",
    "    'CATEGORIA_TARGET': [\n",
    "        \"Cartão - Cobrança\", \"Conta - Pix\", \"Atendimento - Postura\", \n",
    "        \"Cartão - Falha\", \"Investimento - Aplicação\", \"Atendimento - Postura\"\n",
    "    ] * 100\n",
    "})\n",
    "\n",
    "print(\"Gerando vetores do histórico (Isso o BERT faz bem)...\")\n",
    "X_vectors = generate_embeddings_batched(\n",
    "    df_historico['TEXTO_CLIENTE'].tolist(), \n",
    "    tokenizer, \n",
    "    model\n",
    ")\n",
    "y_labels = df_historico['CATEGORIA_TARGET']\n",
    "\n",
    "# ======================================================\n",
    "# 2. TREINAR O CLASSIFICADOR (A Mágica)\n",
    "# ======================================================\n",
    "print(\"Treinando o cérebro da categorização...\")\n",
    "\n",
    "# Divide em treino e teste para validar se está bom\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vectors, y_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Usamos um MLPClassifier (Multi-Layer Perceptron).\n",
    "# É uma rede neural simples que roda na CPU e aprende muito bem com vetores do BERT.\n",
    "clf = MLPClassifier(\n",
    "    hidden_layer_sizes=(256, 128), # Duas camadas de neurônios\n",
    "    max_iter=500, \n",
    "    activation='relu', \n",
    "    solver='adam', \n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# ======================================================\n",
    "# 3. AVALIAR\n",
    "# ======================================================\n",
    "print(\"\\nRelatório de Qualidade do Modelo:\")\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# ======================================================\n",
    "# 4. SALVAR PARA USAR DEPOIS\n",
    "# ======================================================\n",
    "# Salva o classificador treinado (pesa poucos MBs)\n",
    "joblib.dump(clf, 'meu_classificador_treinado.pkl')\n",
    "print(\"Classificador salvo com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041fd17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega o classificador que você treinou\n",
    "clf_carregado = joblib.load('meu_classificador_treinado.pkl')\n",
    "\n",
    "# 1. Carrega os novos dados (sem classificação)\n",
    "df_novos = pd.read_csv(\"novas_reclamacoes.csv\") \n",
    "\n",
    "# 2. Gera os vetores com o BERT (igual antes)\n",
    "vetores_novos = generate_embeddings_batched(\n",
    "    df_novos['TEXTO_CLIENTE'].tolist(), \n",
    "    tokenizer, \n",
    "    model\n",
    ")\n",
    "\n",
    "# 3. Pede para o classificador prever\n",
    "predicoes = clf_carregado.predict(vetores_novos)\n",
    "\n",
    "# 4. Salva no Excel\n",
    "df_novos['CATEGORIA_PREDITA'] = predicoes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
